# Autogenerated by onnx-pytorch.

import glob
import os
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model_U(nn.Module):
  def __init__(self):
    super(Model_U, self).__init__()
    self._vars = nn.ParameterDict()
    self._regularizer_params = []
    for b in glob.glob(
        os.path.join(os.path.dirname(__file__), "variables/v_marblenet", "*.npy")):
      v = torch.from_numpy(np.expand_dims(np.load(b),-1))
      requires_grad = v.dtype.is_floating_point or v.dtype.is_complex
      self._vars[os.path.basename(b)[:-4]] = nn.Parameter(v, requires_grad=requires_grad)
    self.n_Conv_0 = nn.Conv2d(**{'groups': 64, 'dilation': 1, 'out_channels': 64, 'padding': (5,0), 'kernel_size': (11,1), 'stride': 1, 'in_channels': 64, 'bias': False})
    self.n_Conv_0.weight.data = self._vars["encoder_encoder_0_mconv_0_conv_weight"]
    self.n_Conv_1 = nn.Conv2d(**{'groups': 1, 'dilation': 1, 'out_channels': 128, 'padding': (0,0), 'kernel_size': (1,1), 'stride': 1, 'in_channels': 64, 'bias': True})
    self.n_Conv_1.weight.data = self._vars["t_143"]
    self.n_Conv_1.bias.data = self._vars["t_144"].squeeze(-1)
    self.n_Conv_3 = nn.Conv2d(**{'groups': 128, 'dilation': 1, 'out_channels': 128, 'padding': (6,0), 'kernel_size': (13,1), 'stride': 1, 'in_channels': 128, 'bias': False})
    self.n_Conv_3.weight.data = self._vars["encoder_encoder_1_mconv_0_conv_weight"]
    self.n_Conv_4 = nn.Conv2d(**{'groups': 1, 'dilation': 1, 'out_channels': 64, 'padding': (0,0), 'kernel_size': (1,1), 'stride': 1, 'in_channels': 128, 'bias': True})
    self.n_Conv_4.weight.data = self._vars["t_146"]
    self.n_Conv_4.bias.data = self._vars["t_147"].squeeze(-1)
    self.n_Conv_6 = nn.Conv2d(**{'groups': 64, 'dilation': 1, 'out_channels': 64, 'padding': (6,0), 'kernel_size': (13,1), 'stride': 1, 'in_channels': 64, 'bias': False})
    self.n_Conv_6.weight.data = self._vars["encoder_encoder_1_mconv_5_conv_weight"]
    self.n_Conv_7 = nn.Conv2d(**{'groups': 1, 'dilation': 1, 'out_channels': 64, 'padding': (0,0), 'kernel_size': (1,1), 'stride': 1, 'in_channels': 64, 'bias': True})
    self.n_Conv_7.weight.data = self._vars["t_149"]
    self.n_Conv_7.bias.data = self._vars["t_150"].squeeze(-1)
    self.n_Conv_8 = nn.Conv2d(**{'groups': 1, 'dilation': 1, 'out_channels': 64, 'padding': (0,0), 'kernel_size': (1,1), 'stride': 1, 'in_channels': 128, 'bias': True})
    self.n_Conv_8.weight.data = self._vars["t_152"]
    self.n_Conv_8.bias.data = self._vars["t_153"].squeeze(-1)
    self.n_Conv_11 = nn.Conv2d(**{'groups': 64, 'dilation': 1, 'out_channels': 64, 'padding': (7,0), 'kernel_size': (15,1), 'stride': 1, 'in_channels': 64, 'bias': False})
    self.n_Conv_11.weight.data = self._vars["encoder_encoder_2_mconv_0_conv_weight"]
    self.n_Conv_12 = nn.Conv2d(**{'groups': 1, 'dilation': 1, 'out_channels': 64, 'padding': (0,0), 'kernel_size': (1,1), 'stride': 1, 'in_channels': 64, 'bias': True})
    self.n_Conv_12.weight.data = self._vars["t_155"]
    self.n_Conv_12.bias.data = self._vars["t_156"].squeeze(-1)
    self.n_Conv_14 = nn.Conv2d(**{'groups': 64, 'dilation': 1, 'out_channels': 64, 'padding': (7,0), 'kernel_size': (15,1), 'stride': 1, 'in_channels': 64, 'bias': False})
    self.n_Conv_14.weight.data = self._vars["encoder_encoder_2_mconv_5_conv_weight"]
    self.n_Conv_15 = nn.Conv2d(**{'groups': 1, 'dilation': 1, 'out_channels': 64, 'padding': (0,0), 'kernel_size': (1,1), 'stride': 1, 'in_channels': 64, 'bias': True})
    self.n_Conv_15.weight.data = self._vars["t_158"]
    self.n_Conv_15.bias.data = self._vars["t_159"].squeeze(-1)
    self.n_Conv_16 = nn.Conv2d(**{'groups': 1, 'dilation': 1, 'out_channels': 64, 'padding': (0,0), 'kernel_size': (1,1), 'stride': 1, 'in_channels': 64, 'bias': True})
    self.n_Conv_16.weight.data = self._vars["t_161"]
    self.n_Conv_16.bias.data = self._vars["t_162"].squeeze(-1)

  def forward(self, *inputs):
    audio_signal, = inputs
    t_86 = self.n_Conv_0(audio_signal)
    t_142 = self.n_Conv_1(t_86)
    t_89 = F.relu(t_142)
    t_90 = self.n_Conv_3(t_89)
    t_145 = self.n_Conv_4(t_90)
    t_93 = F.relu(t_145)
    t_94 = self.n_Conv_6(t_93)
    t_148 = self.n_Conv_7(t_94)
    t_151 = self.n_Conv_8(t_89)
    t_99 = torch.add(t_148, t_151)
    t_100 = F.relu(t_99)
    t_101 = self.n_Conv_11(t_100)
    t_154 = self.n_Conv_12(t_101)
    t_104 = F.relu(t_154)
    t_105 = self.n_Conv_14(t_104)
    t_157 = self.n_Conv_15(t_105)
    t_160 = self.n_Conv_16(t_100)
    t_110 = torch.add(t_157, t_160)
    t_111 = F.relu(t_110)

    t_111 = t_111.squeeze(-1)
    return t_111

class Model_D(nn.Module):
  def __init__(self):
    super(Model_D, self).__init__()
    self._vars = nn.ParameterDict()
    self._regularizer_params = []
    for b in glob.glob(
        os.path.join(os.path.dirname(__file__), "variables/v_marblenet", "*.npy")):
      v = torch.from_numpy(np.load(b))
      requires_grad = v.dtype.is_floating_point or v.dtype.is_complex
      self._vars[os.path.basename(b)[:-4]] = nn.Parameter(v, requires_grad=requires_grad)
    self.n_Conv_19 = nn.Conv1d(**{'groups': 64, 'dilation': 1, 'out_channels': 64, 'padding': 8, 'kernel_size': 17, 'stride': 1, 'in_channels': 64, 'bias': False})
    self.n_Conv_19.weight.data = self._vars["encoder_encoder_3_mconv_0_conv_weight"]
    self.n_Conv_20 = nn.Conv1d(**{'groups': 1, 'dilation': 1, 'out_channels': 64, 'padding': 0, 'kernel_size': 1, 'stride': 1, 'in_channels': 64, 'bias': True})
    self.n_Conv_20.weight.data = self._vars["t_164"]
    self.n_Conv_20.bias.data = self._vars["t_165"]
    self.n_Conv_22 = nn.Conv1d(**{'groups': 64, 'dilation': 1, 'out_channels': 64, 'padding': 8, 'kernel_size': 17, 'stride': 1, 'in_channels': 64, 'bias': False})
    self.n_Conv_22.weight.data = self._vars["encoder_encoder_3_mconv_5_conv_weight"]
    self.n_Conv_23 = nn.Conv1d(**{'groups': 1, 'dilation': 1, 'out_channels': 64, 'padding': 0, 'kernel_size': 1, 'stride': 1, 'in_channels': 64, 'bias': True})
    self.n_Conv_23.weight.data = self._vars["t_167"]
    self.n_Conv_23.bias.data = self._vars["t_168"]
    self.n_Conv_24 = nn.Conv1d(**{'groups': 1, 'dilation': 1, 'out_channels': 64, 'padding': 0, 'kernel_size': 1, 'stride': 1, 'in_channels': 64, 'bias': True})
    self.n_Conv_24.weight.data = self._vars["t_170"]
    self.n_Conv_24.bias.data = self._vars["t_171"]
    self.n_Conv_27 = nn.Conv1d(**{'groups': 64, 'dilation': 2, 'out_channels': 64, 'padding': 28, 'kernel_size': 29, 'stride': 1, 'in_channels': 64, 'bias': False})
    self.n_Conv_27.weight.data = self._vars["encoder_encoder_4_mconv_0_conv_weight"]
    self.n_Conv_28 = nn.Conv1d(**{'groups': 1, 'dilation': 1, 'out_channels': 128, 'padding': 0, 'kernel_size': 1, 'stride': 1, 'in_channels': 64, 'bias': True})
    self.n_Conv_28.weight.data = self._vars["t_173"]
    self.n_Conv_28.bias.data = self._vars["t_174"]
    self.n_Conv_30 = nn.Conv1d(**{'groups': 1, 'dilation': 1, 'out_channels': 128, 'padding': 0, 'kernel_size': 1, 'stride': 1, 'in_channels': 128, 'bias': True})
    self.n_Conv_30.weight.data = self._vars["t_176"]
    self.n_Conv_30.bias.data = self._vars["t_177"]
  def forward(self, *inputs):
    t_111, = inputs
    t_112 = self.n_Conv_19(t_111)
    t_163 = self.n_Conv_20(t_112)
    t_115 = F.relu(t_163)
    t_116 = self.n_Conv_22(t_115)
    t_166 = self.n_Conv_23(t_116)
    t_169 = self.n_Conv_24(t_111)
    t_121 = torch.add(t_166, t_169)
    t_122 = F.relu(t_121)
    t_123 = self.n_Conv_27(t_122)
    t_172 = self.n_Conv_28(t_123)
    t_126 = F.relu(t_172)
    t_175 = self.n_Conv_30(t_126)
    t_129 = F.relu(t_175)
    t_130 = torch.tensor(t_129.shape, device=t_129.device)
    t_132 = self.gather(t_130, 0, self._vars["t_131"])
    t_133 = torch.tensor(t_129.shape, device=t_129.device)
    t_135 = self.gather(t_133, 0, self._vars["t_134"])
    t_136 = F.avg_pool1d(t_129, **{'kernel_size': t_129.shape[-1:]})
    t_137 = torch.unsqueeze(t_132, 0)
    t_138 = torch.unsqueeze(t_135, 0)
    t_139 = torch.cat((t_137, t_138), **{'dim': 0})
    t_140 = torch.reshape(t_136, [s if s != 0 else t_136.shape[i] for i, s in enumerate(t_139)])
    logits = 1.0 * torch.matmul(t_140, torch.transpose(self._vars["decoder_decoder_layers_0_weight"], 0, 1)) + 1.0 * self._vars["decoder_decoder_layers_0_bias"]
    return logits

  def gather(self, input, dim, indices, **kwargs):
    shape_l, shape_r = list(input.shape), list(indices.shape)
    indices = indices.flatten().to(device=indices.device, dtype=torch.int64)
    for r in range(0, dim):
      indices = indices.unsqueeze(0)
    for r in range(dim, len(shape_l) - 1):
      indices = indices.unsqueeze(-1)
    indices = indices.expand(*(shape_l[:dim] + [int(np.prod(shape_r))] + shape_l[dim + 1:]))
    indices = torch.where(indices >= 0, indices, indices + shape_l[dim])
    output = torch.gather(input, dim, indices)
    output = torch.reshape(output, shape_l[:dim] + shape_r + shape_l[dim + 1:])
    return output
